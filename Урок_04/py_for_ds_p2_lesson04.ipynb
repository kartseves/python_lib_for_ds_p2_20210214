{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Вопрос 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Расскажите, как работает регуляризация в решающих деревьях, какие параметры мы штрафуем в данных алгоритмах?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Регуляризация в решающих деревьях заключается в ограничении определенных параметров дерева:\n",
    "\n",
    "* **Глубина дерева** — параметр, который ограничивает максимальный рост дерева (деревья принятия решений растут в глубину). Этот параметр позволяет уменьшить переобучение, но ограничивает количество переменных для каждого конкретного листа.\n",
    "* **Минимальный вес листа** — параметр, который ограничивает рост дерева, когда следующее деление листа приводит к тому, что хотя бы в одном из них слишком мало наблюдений, что делало бы его слишком специфичным.\n",
    "\n",
    "Также для регрессионных деревьев можно настраивать коэффициенты для линейной регуляризации, которые используются в их листах.\n",
    "\n",
    "**Леса деревьев принятия решений** — более сложная модель. Для нее настраиваются параметры регуляризации, которые встречаются во многих нейросетях и являются универсальными для большинства итеративных моделей обучения:\n",
    "\n",
    "* **Скорость обучения (learning rate)** — коэффициент, который показывает, насколько подробно нужно уточнять свои результаты с каждым шагом. Если сделать его слишком низким, то понадобится больше итераций, чтобы прийти к хорошему решению. Но есть риск, что остановиться вовремя не получится, и модель выучит данные слишком подробно, что приведет к переобучению. Если же, напротив, этот параметр будет слишком высоким, модель сможет выйти на хорошую точность за меньшее количество итераций. Но ей будет сложно приблизиться к лучшему результату, и она останется недообученной.\n",
    "* **Отсев (dropout)** — параметр, которым задается относительная часть всех данных, скрытая случайным образом во время обучения. Скрывая часть данных от моделей, мы отнимаем у них возможность использовать всю вариативность данных, чтоб выучить их наизусть. Слишком высокий отсев может скрыть искомую зависимость между признаками.\n",
    "\n",
    "**Ранняя остановка (early stopping)** — это стратегия, при которой мы возвращаемся к последней лучшей итерации в случае, если после нескольких итераций подряд точность модели на скрытых валидационных данных не улучшилась. Это универсальный метод. Чтобы его применять, нужно достаточное количество валидационных данных для обучения."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Вопрос 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### По какому принципу рассчитывается \"важность признака (feature_importance)\" в ансамблях деревьев?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "При каждом разбиении в каждом дереве ансамбля деревьев **улучшение критерия разделения** (например, неопределенности Джини) — **это показатель важности**, связанный с переменной разделения (признака), и накапливается он по всем деревьям ансамбля деревьев отдельно для каждой переменной."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Среднее снижение точности, вызываемое переменной (признаком), определяется во время фазы вычисления out-of-bag ошибки.\n",
    "\n",
    "Чем больше уменьшается точность предсказаний из-за исключения (или перестановки) одной переменной, тем важнее эта переменная, и поэтому переменные с бо́льшим средним уменьшением точности более важны для классификации данных.\n",
    "\n",
    "Среднее уменьшение неопределенности Джини (или ошибки mse в задачах регрессии) является мерой того, как каждая переменная способствует однородности узлов и листьев в окончательной модели ансамбля деревьев.\n",
    "\n",
    "Каждый раз, когда отдельная переменная используется для разбиения узла, неопределенность Джини для дочерних узлов рассчитывается и сравнивается с коэффициентом исходного узла. Неопределенность Джини является мерой однородности от 0 (однородной) до 1 (гетерогенной). Изменения в значении критерия разделения суммируются для каждой переменной и нормируются в конце вычисления.\n",
    "\n",
    "Переменные, которые приводят к узлам с более высокой чистотой, имеют более высокое снижение коэффициента Джини."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
